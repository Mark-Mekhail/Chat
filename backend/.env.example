# Model settings
MODEL_PATH=/app/models/llama-2-7b-chat.gguf
N_CTX=2048
N_THREADS=4   # Number of CPU threads to use
N_GPU_LAYERS=-1  # -1 means use all available GPU layers

# Server settings
DEBUG=False
LOG_LEVEL=INFO
API_VERSION=1.0.0  # Corresponds to the API version in config.py

# Security settings
# CORS_ORIGINS=http://localhost:3000,https://example.com  # Uncomment for production to restrict CORS
