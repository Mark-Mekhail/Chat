services:
  backend:
    build: ./backend
    container_name: ai-chat-backend
    ports:
      - "8000:8000"
    volumes:
      - ./models:/app/models
    environment:
      - MODEL_DIR=/app/models
      - MODEL_PATH=/app/models/llama-2-7b-chat.gguf
      - PORT=8000
    restart: unless-stopped

  frontend:
    build: ./frontend
    container_name: ai-chat-frontend
    ports:
      - "80:80"
    depends_on:
      - backend
    restart: unless-stopped

volumes:
  models:
    driver: local